trigger:
  - pre_pr_cloud-db-integration-*

schedules:
- cron: "0 12 * * 0"
  displayName: Weekly Sunday build
  branches:
    include:
    - develop

variables:
  isMain: $[eq(variables['Build.SourceBranch'], 'refs/heads/main')]
  isDevelop: $[eq(variables['Build.SourceBranch'], 'refs/heads/develop')]
  GE_USAGE_STATISTICS_URL: "https://qa.stats.greatexpectations.io/great_expectations/v1/usage_statistics"

stages:
  - stage: scope_check
    pool:
      vmImage: 'ubuntu-20.04'
    jobs:
      - job: changes
        steps:
          - task: ChangedFiles@1
            name: CheckChanges
            inputs:
              verbose: true
              rules: |
                [ContribChanged]
                contrib/**

                [ExperimentalChanged]
                contrib/experimental/**

                [DocsChanged]
                docs/**
                tests/integration/docusaurus/**
                tests/integration/fixtures/**
                tests/test_sets/**

                [GEChanged]
                great_expectations/**
                tests/**
                /*.txt
                /*.yml

  - stage: lint
    dependsOn: scope_check
    pool:
      vmImage: 'ubuntu-latest'

    jobs:
      - job: lint
        condition: or(eq(stageDependencies.scope_check.changes.outputs['CheckChanges.GEChanged'], true), eq(variables.isMain, true))
        steps:
          - task: UsePythonVersion@0
            inputs:
              versionSpec: 3.7
            displayName: 'Use Python 3.7'

          - script: |
              pip install isort[requirements]==5.4.2 flake8==3.8.3 black==20.8b1 pyupgrade==2.7.2
              EXIT_STATUS=0
              isort . --check-only --skip docs/ || EXIT_STATUS=$?
              black --check --exclude docs/ . || EXIT_STATUS=$?
              flake8 great_expectations/core || EXIT_STATUS=$?
              pyupgrade --py3-plus || EXIT_STATUS=$?
              exit $EXIT_STATUS

#  - stage: cloud_db_integration_expectations
#    pool:
#      vmImage: 'ubuntu-latest'
#
#    dependsOn: [scope_check, lint, cloud_db_integration_expectations_cfe]
#    jobs:
#      - job: bigquery_expectations
#        timeoutInMinutes: 0 # Maximize the time that pipelines remain open (6 hours currently)
#        condition: or(eq(stageDependencies.scope_check.changes.outputs['CheckChanges.GEChanged'], true), eq(variables.isMain, true))
#
#        variables:
#          python.version: '3.8'
#
#        steps:
#          - task: UsePythonVersion@0
#            inputs:
#              versionSpec: '$(python.version)'
#            displayName: 'Use Python $(python.version)'
#
#          - bash: python -m pip install --upgrade pip==20.2.4
#            displayName: 'Update pip'
#
#          - script: |
#              pip install -r requirements-dev.txt
#
#            displayName: 'Install dependencies'
#
#          - task: DownloadSecureFile@1
#            name: gcp_authkey
#            displayName: 'Download Google Service Account'
#            inputs:
#              secureFile: 'superconductive-service-acct.json'
#              retryCount: '2'
#
#          - script: |
#              pip install pytest pytest-azurepipelines
#              pytest -v --no-spark --no-postgresql --bigquery --napoleon-docstrings --junitxml=junit/test-results.xml --cov=. --cov-report=xml --cov-report=html --ignore=tests/cli --ignore=tests/integration/usage_statistics tests/test_definitions/test_expectations.py
#
#            displayName: 'pytest'
#            env:
#              GOOGLE_APPLICATION_CREDENTIALS: $(gcp_authkey.secureFilePath)
#              GCP_PROJECT: $(GCP_PROJECT)
#              GCP_BIGQUERY_DATASET: $(GCP_BIGQUERY_DATASET)

  - stage: cloud_db_integration_expectations_cfe
    pool:
      vmImage: 'ubuntu-latest'

    dependsOn: [scope_check, lint]

    jobs:
      - job: bigquery_expectations_cfe
        timeoutInMinutes: 0 # Maximize the time that pipelines remain open (6 hours currently)
        condition: or(eq(stageDependencies.scope_check.changes.outputs['CheckChanges.GEChanged'], true), eq(variables.isMain, true))

        variables:
          python.version: '3.8'

        steps:
          # delay the execution of the second stages so that we do not hit the rate limit for BigQuery
          #- bash: sleep 5m
          #  displayName: Delay for BigQuery rate limit

          - task: UsePythonVersion@0
            inputs:
              versionSpec: '$(python.version)'
            displayName: 'Use Python $(python.version)'

          - bash: python -m pip install --upgrade pip==20.2.4
            displayName: 'Update pip'

          - script: |
              pip install -r requirements-dev.txt

            displayName: 'Install dependencies'

          - task: DownloadSecureFile@1
            name: gcp_authkey
            displayName: 'Download Google Service Account'
            inputs:
              secureFile: 'superconductive-service-acct.json'
              retryCount: '2'

          - script: |
              pip install pytest pytest-azurepipelines
              pytest -v --no-spark --no-postgresql --bigquery --napoleon-docstrings --junitxml=junit/test-results.xml --cov=. --cov-report=xml --cov-report=html --ignore=tests/cli --ignore=tests/integration/usage_statistics tests/test_definitions/test_expectations_cfe.py

            displayName: 'pytest'
            env:
              GOOGLE_APPLICATION_CREDENTIALS: $(gcp_authkey.secureFilePath)
              GCP_PROJECT: $(GCP_PROJECT)
              GCP_BIGQUERY_DATASET: $(GCP_BIGQUERY_DATASET)
